{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation models + Timm encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditionally, most classification models, like the classic VGG and ResNet models, followed a 5-\"stage\" architecture. In each stage, these models increased the number of channels in the features while **halving the spatial resolution**. Classic decoders for semantic segmentations were designed with this architecture in mind, expecting features with progressively reduced spatial resolutions at each stage. For instance, the 5 feature maps should be at resolutions: \n",
    "\n",
    " - h // 2,  w // 2\n",
    " - h // 4,  w // 4\n",
    " - h // 8,  w // 8\n",
    " - h // 16, w // 16 \n",
    " - h // 32, w // 32\n",
    "\n",
    "Recent advancements in convolutional and transformer models architectures have significantly diversified intermediate representations of features. Modern models might produce feature maps with uniform spatial resolutions, such as multiple maps all at \\[h // 16, w // 16\\] or \\[h // 8, w // 8\\], or fewer/greater number of feature maps, such as just 3 instead of the traditional 5.\n",
    "\n",
    "The [timm](https://github.com/huggingface/pytorch-image-models) library provides detailed feature information for various architectures, allowing us to adapt these features to our standard approach.\n",
    "\n",
    "Hereâ€™s how we can adapt to these changes:\n",
    "\n",
    "1) **Feature Selection:** We should choose the appropriate feature maps when the feature extractor model provides a larger number of output features than needed.\n",
    "2) **Feature Padding:** Add (\"pad\") feature maps when the number of output features is less than required.\n",
    "3) **Spatial Resolution Adjustment:** Adjust the spatial resolutions to match the classic scheme [h/2, w/2; h/4, w/4; h/8, w/8; h/16, w/16; h/32, w/32], which is expected by most decoders.\n",
    "4) **Channel Reduction:** Optionally reduce the number of channels in the features to create a lighter, more efficient model.\n",
    "\n",
    "By implementing these adaptations, we can ensure compatibility with various decoder architectures and leverage the full potential of modern feature extractor models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we are going to explore the following encoder cases:\n",
    "\n",
    " - [Traditional encoder with 5 stages](#traditional-encoder)\n",
    " - [Selecting features from encoder](#feature-selection)\n",
    " - [Encoder with less than 5 blocs](#encoders-with-less-than-5-blocks)\n",
    " - [Specifying number of channels for encoder](#specifying-number-of-output-channels-for-encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/projects/segmentation_models.pytorch/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For traditional encoder, with standard number of features and standard spatial feature resolutions, the approach is simple, we just take all features and pass them as is to the decoder. As you can see below, timm model output features, selected features and adapted features are the same in shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index / module :    Timm model features  ->     Selected features   ->     Adapted features   \n",
      "-----------------------------------------------------------------------------------------------\n",
      " 0       (act1):         64, hw /  2     ->         64, hw /  2     ->         64, hw /  2    \n",
      " 1     (layer1):         64, hw /  4     ->         64, hw /  4     ->         64, hw /  4    \n",
      " 2     (layer2):        128, hw /  8     ->        128, hw /  8     ->        128, hw /  8    \n",
      " 3     (layer3):        256, hw / 16     ->        256, hw / 16     ->        256, hw / 16    \n",
      " 4     (layer4):        512, hw / 32     ->        512, hw / 32     ->        512, hw / 32    \n"
     ]
    }
   ],
   "source": [
    "model = smp.Unet(encoder_name=\"tu-resnet18\")\n",
    "print(model.encoder.features_info_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Features Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also customize model with `timm` encoder by reducing its depth and specifying which features to use.\n",
    "For example, if you set `encoder_depth=3` the first 3 feature maps of encoder will be used, and the rest will be ignored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index / module :    Timm model features  ->     Selected features   ->     Adapted features   \n",
      "-----------------------------------------------------------------------------------------------\n",
      " 0       (act1):         64, hw /  2     ->         64, hw /  2     ->         64, hw /  2    \n",
      " 1     (layer1):         64, hw /  4     ->         64, hw /  4     ->         64, hw /  4    \n",
      " 2     (layer2):        128, hw /  8     ->        128, hw /  8     ->        128, hw /  8    \n",
      " 3     (layer3):        256, hw / 16     -x\n",
      " 4     (layer4):        512, hw / 32     -x\n"
     ]
    }
   ],
   "source": [
    "model = smp.Unet(encoder_name=\"tu-resnet18\", encoder_depth=3)\n",
    "print(model.encoder.features_info_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can also chose last encoder features instead of the first ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index / module :    Timm model features  ->     Selected features   ->     Adapted features   \n",
      "-----------------------------------------------------------------------------------------------\n",
      " 0       (act1):         64, hw /  2     -x\n",
      " 1     (layer1):         64, hw /  4     -x\n",
      " 2     (layer2):        128, hw /  8     ->        128, hw /  8     ->        128, hw /  2    \n",
      " 3     (layer3):        256, hw / 16     ->        256, hw / 16     ->        256, hw /  4    \n",
      " 4     (layer4):        512, hw / 32     ->        512, hw / 32     ->        512, hw /  8    \n"
     ]
    }
   ],
   "source": [
    "model = smp.Unet(encoder_name=\"tu-resnet18\", encoder_depth=3, encoder_indices=\"last\")\n",
    "print(model.encoder.features_info_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, even specify particular indices of encoder features to be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index / module :    Timm model features  ->     Selected features   ->     Adapted features   \n",
      "-----------------------------------------------------------------------------------------------\n",
      " 0       (act1):         64, hw /  2     ->         64, hw /  2     ->         64, hw /  2    \n",
      " 1     (layer1):         64, hw /  4     -x\n",
      " 2     (layer2):        128, hw /  8     ->        128, hw /  8     ->        128, hw /  4    \n",
      " 3     (layer3):        256, hw / 16     -x\n",
      " 4     (layer4):        512, hw / 32     ->        512, hw / 32     ->        512, hw /  8    \n"
     ]
    }
   ],
   "source": [
    "model = smp.Unet(encoder_name=\"tu-resnet18\", encoder_depth=3, encoder_indices=[0, 2, 4])\n",
    "print(model.encoder.features_info_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, final encoder features are always adapted to have reductions `2^1 .. 2^encoder_depth`, for above examples, with `encoder_depth=3` reductions are [2, 4, 8]. For `encoder_depth=4` reductions will be [2, 4, 8, 16]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoders with less than 5 blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For encoders with less than 5 feature blocks, the following approaches applied:\n",
    " - Depth is adjusted to match maximum features reduction.\n",
    " - Missing features are filled with `dymmy` feature maps, that have 0 dims and not influence training/inference somehow.\n",
    "\n",
    "**How the depth is adjusted?**\n",
    "\n",
    "We take the maximum depth by the number of feature maps or by reduction. Lets see on examples:\n",
    " - Encoder with 3 feature maps [16, 16, 16] reductions. Maximum reduction 16 = 2^**4** -> so, the encoder depth will be adjusted to 4.\n",
    " - Encoder with 3 feature maps [4, 4, 4] reductions. Maximum reduction 4 = 2^**2**, however, the number of feature maps is 3 -> so, the encoder depth will be adjusted to 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 1:\n",
    "Encoder with 4 feature maps and reductions [4, 8, 16, 32]. Maximum reduction is 32 = 2^**5** -> encoder depth is 5. However, the number of features is just 4, feature map with reduction `2` is missed. This feature map will be filled with `dummy` feature of shape `[0, h / 2, w / 2]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-08 22:48:48.695\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msegmentation_models_pytorch.encoders.timm_universal\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m172\u001b[0m - \u001b[34m\u001b[1mEncoder has 1 dummy feature(s), because the real number of features (4) is less than specified encoder depth (5).\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index / module :    Timm model features  ->     Selected features   ->     Adapted features   \n",
      "-----------------------------------------------------------------------------------------------\n",
      " x     (-none-):                         ->          0, hw /  2     ->          0, hw /  2    \n",
      " 0   (stages.0):         48, hw /  4     ->         48, hw /  4     ->         48, hw /  4    \n",
      " 1   (stages.1):         96, hw /  8     ->         96, hw /  8     ->         96, hw /  8    \n",
      " 2   (stages.2):        224, hw / 16     ->        224, hw / 16     ->        224, hw / 16    \n",
      " 3   (stages.3):        448, hw / 32     ->        448, hw / 32     ->        448, hw / 32    \n"
     ]
    }
   ],
   "source": [
    "model = smp.Unet(encoder_name=\"tu-efficientformer_l1\", encoder_weights=None)\n",
    "print(model.encoder.features_info_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 2:\n",
    "\n",
    "Despite specified `encoder_depth=5` encoder has only 3 feature maps with [16, 16, 16] reductions. Maximum reduction is 16 = 2^**4** -> so, the encoder depth will be adjusted to **4**.\n",
    "One `dummy` feature is created of shape `[0, h / 2, w / 2]`. Other features are resized to match [4, 8, 16] reductions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-08 22:48:49.088\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msegmentation_models_pytorch.encoders.timm_universal\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1mEncoder depth is adjusted to `encoder_depth=4` to match `timm` model features reductions [16, 16, 16].\u001b[0m\n",
      "\u001b[32m2024-06-08 22:48:49.089\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msegmentation_models_pytorch.encoders.timm_universal\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m172\u001b[0m - \u001b[34m\u001b[1mEncoder has 1 dummy feature(s), because the real number of features (3) is less than specified encoder depth (4).\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index / module :    Timm model features  ->     Selected features   ->     Adapted features   \n",
      "-----------------------------------------------------------------------------------------------\n",
      " x     (-none-):                         ->          0, hw /  2     ->          0, hw /  2    \n",
      "21  (blocks.21):        192, hw / 16     ->        192, hw / 16     ->        192, hw /  4    \n",
      "22  (blocks.22):        192, hw / 16     ->        192, hw / 16     ->        192, hw /  8    \n",
      "23  (blocks.23):        192, hw / 16     ->        192, hw / 16     ->        192, hw / 16    \n"
     ]
    }
   ],
   "source": [
    "model = smp.Unet(encoder_name=\"tu-xcit_tiny_24_p16_224\", encoder_depth=5)\n",
    "print(model.encoder.features_info_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying number of output channels for encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default adapted features wil have the same number of channels as selected features. You can change this by passing `encoder_channels` argument to the encoder constructor. See the shape difference between in selected and adapted features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index / module :    Timm model features  ->     Selected features   ->     Adapted features   \n",
      "-----------------------------------------------------------------------------------------------\n",
      " 0       (act1):         64, hw /  2     ->         64, hw /  2     ->         64, hw /  2    \n",
      " 1     (layer1):         64, hw /  4     ->         64, hw /  4     ->         64, hw /  4    \n",
      " 2     (layer2):        128, hw /  8     ->        128, hw /  8     ->         64, hw /  8    \n",
      " 3     (layer3):        256, hw / 16     ->        256, hw / 16     ->         64, hw / 16    \n",
      " 4     (layer4):        512, hw / 32     ->        512, hw / 32     ->         64, hw / 32    \n"
     ]
    }
   ],
   "source": [
    "model = smp.Unet(encoder_name=\"tu-resnet18\", encoder_channels=[64, 64, 64, 64, 64])\n",
    "print(model.encoder.features_info_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of channels is changed by applying `1x1` convolution without any non-linearity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
